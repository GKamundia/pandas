{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Pandas Series and DataFrames - Comprehensive Lab\n",
    "\n",
    "## Introduction\n",
    "This notebook provides a hands-on exploration of pandas Series and DataFrames, focusing on:\n",
    "- Data cleaning and transformation\n",
    "- Using `.map()` and `.apply()` methods\n",
    "- Manipulating DataFrame structure\n",
    "- Working with indices and data types\n",
    "- Grouping and analysis techniques\n",
    "\n",
    "Let's dive in and build practical data skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set some display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "# Increase the number of rows pandas will display\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Optionally, reset back to default later if needed\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reading in the turnstile data\n",
    "df = pd.read_csv('turnstile_180901.txt')\n",
    "\n",
    "print(f\"DataFrame Shape: {df.shape} (rows, columns)\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Inspection\n",
    "# Looking at data types and checking for missing values\n",
    "print(\"DataFrame info (data types and non-null counts):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning Column Names - Using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original column names\n",
    "print(\"Original column names:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Understanding list comprehension for renaming columns:\n",
    "# - 'col' is a temporary variable that takes each value from df.columns\n",
    "# - .lower() converts each column name to lowercase\n",
    "# - We create a new list of lowercase names and assign it back to df.columns\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "print(\"\\nLowercase column names:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Advanced renaming using a custom function\n",
    "def clean_column_name(name):\n",
    "    \"\"\"Function to standardize column names\"\"\"\n",
    "    # Convert to lowercase, replace spaces with underscores\n",
    "    cleaned = name.lower().replace(' ', '_').replace('/', '_')\n",
    "    return cleaned\n",
    "\n",
    "# We could also use this approach:\n",
    "#df.columns = [clean_column_name(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what the column names actually look like and if they have whitespace\n",
    "print(\"Column names with their string representation (to see any whitespace):\")\n",
    "for col in df.columns:\n",
    "    print(f\"'{col}' (length: {len(col)})\")\n",
    "\n",
    "# Strip whitespace from all column names\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"\\nColumn names after stripping whitespace:\")\n",
    "print(df.columns.tolist()) #.tolist() converts the Index object to a list for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .map() - Used for mapping values in a Series based on a dictionary or function\n",
    "# Let's create a day of week mapping first by converting dates to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "# Map day numbers to day names\n",
    "day_mapping = {\n",
    "    0: 'Monday', \n",
    "    1: 'Tuesday', \n",
    "    2: 'Wednesday', \n",
    "    3: 'Thursday', \n",
    "    4: 'Friday', \n",
    "    5: 'Saturday', \n",
    "    6: 'Sunday'\n",
    "}\n",
    "\n",
    "# Using map to convert day numbers to day names\n",
    "df['day_name'] = df['day_of_week'].map(day_mapping)\n",
    "\n",
    "# Create a weekend indicator using map\n",
    "weekend_map = {0: False, 1: False, 2: False, 3: False, 4: False, 5: True, 6: True}\n",
    "df['is_weekend'] = df['day_of_week'].map(weekend_map)\n",
    "\n",
    "# View the results\n",
    "df[['date', 'day_of_week', 'day_name', 'is_weekend']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction to .map() and .apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .apply() - More versatile than map, works with custom functions\n",
    "# Example 1: Simple apply to a Series\n",
    "# Count the number of characters in the station name\n",
    "df['station_name_length'] = df['station'].apply(len)\n",
    "\n",
    "# Example 2: Apply a more complex function \n",
    "def count_train_lines(line_name):\n",
    "    \"\"\"Counts the number of train lines that pass through a station\"\"\"\n",
    "    # Each character typically represents a different train line\n",
    "    # Remove non-alphabetic characters for better counting\n",
    "    clean_name = ''.join(c for c in line_name if c.isalpha())\n",
    "    return len(clean_name)\n",
    "\n",
    "# Apply our function to the linename column\n",
    "df['num_lines'] = df['linename'].apply(count_train_lines)\n",
    "\n",
    "# Preview the results\n",
    "df[['station', 'station_name_length', 'linename', 'num_lines']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Working with DataFrame Indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default index\n",
    "print(\"Current index:\")\n",
    "print(df.index)\n",
    "\n",
    "# Setting a column as the index\n",
    "df_indexed = df.set_index('linename')\n",
    "print(\"\\nAfter setting 'linename' as the index:\")\n",
    "print(df_indexed.index)\n",
    "df_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index (turning the index back into a regular column)\n",
    "df_reset = df_indexed.reset_index()\n",
    "print(\"After resetting the index:\")\n",
    "print(df_reset.index)\n",
    "df_reset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read on your own on Multi-level Indexing \n",
    "### What is Multi-level Indexing?\n",
    "Multi-level (or hierarchical) indexing allows you to have multiple levels of indices in your DataFrame. Think of it like organizing data in nested categories:\n",
    "\n",
    "- First level: Major category (in this case, station)\n",
    "- Second level: Sub-category (in this case, linename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-level index and sort it for proper performance\n",
    "df_multi = df.set_index(['station', 'linename']).sort_index()\n",
    "\n",
    "print(\"Multi-level index structure:\")\n",
    "print(df_multi.index.names)  # Shows the names of each level\n",
    "\n",
    "# Display the first few entries\n",
    "print(\"\\nFirst few entries with multi-level index:\")\n",
    "df_multi.head()\n",
    "\n",
    "# Check if the specific combination exists\n",
    "stations = df_multi.index.get_level_values('station').unique()\n",
    "if '59 ST' in stations:\n",
    "    print(\"\\n'59 ST' exists as a station\")\n",
    "    \n",
    "    # Get all the line names for 59 ST\n",
    "    try:\n",
    "        lines_at_59st = df_multi.loc['59 ST'].index.unique()\n",
    "        print(f\"Line names at 59 ST: {lines_at_59st.tolist()}\")\n",
    "        \n",
    "        # If NQR456W exists, show its data\n",
    "        if 'NQR456W' in lines_at_59st:\n",
    "            print(\"\\nData for 59 ST, NQR456W:\")\n",
    "            print(df_multi.loc[('59 ST', 'NQR456W')].head())\n",
    "    except:\n",
    "        print(\"Error accessing station data\")\n",
    "else:\n",
    "    print(\"'59 ST' station not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Changing Data Types\n",
    "\n",
    "## Difference Between `int64` and `int32` in Pandas & NumPy  \n",
    "Both `int64` and `int32` are integer data types, but they differ in **memory usage** and **value range**.\n",
    "\n",
    "| **Data Type** | **Storage Size** | **Value Range** |\n",
    "|--------------|----------------|----------------------------|\n",
    "| `int32` | **4 bytes (32 bits)** | -2,147,483,648 to 2,147,483,647 |\n",
    "| `int64` | **8 bytes (64 bits)** | -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**\n",
    "1. **Memory Usage**  \n",
    "   - `int32` **uses 4 bytes** per value â†’ better for memory efficiency.  \n",
    "   - `int64` **uses 8 bytes** per value â†’ needed for very large numbers.  \n",
    "\n",
    "2. **Number Range**  \n",
    "   - `int32` can store values up to **Â±2.1 billion**.  \n",
    "   - `int64` can store **much larger values**, up to Â±9 quintillion.  \n",
    "\n",
    "3. **Performance Considerations**  \n",
    "   - `int32` operations **use less memory**, making computations slightly faster.  \n",
    "   - `int64` is required for **big data processing** (e.g., large IDs or financial data).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current dtypes:\")\n",
    "print(df.info())\n",
    "\n",
    "# Convert numeric columns to more efficient types\n",
    "# Let's focus on 'entries' and 'exits'\n",
    "print(\"\\nMemory usage before optimization:\")\n",
    "print(df[['entries', 'exits']].memory_usage())\n",
    "\n",
    "# Convert to smaller integer types where appropriate\n",
    "df['entries'] = df['entries'].astype('int32')\n",
    "df['exits'] = df['exits'].astype('int32')\n",
    "\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df[['entries', 'exits']].memory_usage())\n",
    "\n",
    "# Working with dates\n",
    "# We already converted 'date' to datetime\n",
    "print(\"\\nDate range in the dataset:\")\n",
    "print(f\"Start date: {df['date'].min()}\")\n",
    "print(f\"End date: {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Grouping and Aggregating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day of week and calculate average entries/exits\n",
    "# when selecting multiple columns, use a list (double brackets) not a tuple\n",
    "day_grouped = df.groupby('day_name')[['entries', 'exits']].mean()\n",
    "day_grouped = day_grouped.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "print(\"Average entries/exits by day of week:\")\n",
    "day_grouped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by station and count records\n",
    "station_counts = df.groupby('station').size().sort_values(ascending=False)\n",
    "print(\"\\nTop stations by number of records:\")\n",
    "station_counts.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations with groupby\n",
    "station_stats = df.groupby('station').agg({\n",
    "    'entries': ['min', 'max', 'mean'],\n",
    "    'exits': ['min', 'max', 'mean'],\n",
    "    'linename': 'nunique'  # Count unique line names\n",
    "})\n",
    "print(\"\\nMultiple statistics by station:\")\n",
    "station_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sorting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by entries (highest first)\n",
    "df_sorted = df.sort_values('entries', ascending=False)\n",
    "print(\"Top entries records:\")\n",
    "df_sorted[['station', 'date', 'time', 'entries']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns\n",
    "# For example, by station name and then by date\n",
    "df_multi_sorted = df.sort_values(['station', 'date', 'time'])\n",
    "print(\"\\nSorted by station, date, and time:\")\n",
    "df_multi_sorted[['station', 'date', 'time', 'entries']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Creating New Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the hour from the time column\n",
    "# First, let's convert time to a proper time format\n",
    "df['time'] = pd.to_datetime(df['time']).dt.time\n",
    "df['hour'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.hour\n",
    "df[['time', 'hour']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's extract the hour from the time column\n",
    "# # First, let's convert time to a proper time format\n",
    "# df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S')  # Keep full datetime\n",
    "# df['hour'] = df['time'].dt.hour # Extract hour from time\n",
    "# print(\"\\nFirst few rows with extracted hour:\")\n",
    "# df[['time', 'hour']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'time_of_day' column\n",
    "def time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_of_day'] = df['hour'].apply(time_of_day)\n",
    "\n",
    "# View distribution of records by time of day\n",
    "time_of_day_counts = df['time_of_day'].value_counts()\n",
    "print(\"Records by time of day:\")\n",
    "time_of_day_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entries by day of week\n",
    "day_entries = df.groupby('day_name')['entries'].sum()\n",
    "day_entries = day_entries.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "day_entries.plot(kind='bar', color='skyblue')\n",
    "plt.title('Total Entries by Day of Week')\n",
    "plt.ylabel('Total Entries')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend vs. Weekday comparison\n",
    "weekend_comparison = df.groupby('is_weekend')[['entries', 'exits']].sum()\n",
    "weekend_comparison.index = ['Weekday', 'Weekend']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "weekend_comparison.plot(kind='bar', colormap='Set2')\n",
    "plt.title('Entries and Exits: Weekend vs. Weekday')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(['Entries', 'Exits'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of day analysis\n",
    "time_entries = df.groupby('time_of_day')['entries'].sum()\n",
    "time_entries = time_entries.reindex(['Morning', 'Afternoon', 'Evening', 'Night'])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "time_entries.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen', 'coral', 'lavender'])\n",
    "plt.title('Distribution of Entries by Time of Day')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Handling Missing Data and Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows') #\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we did have missing values, here's how we could handle them:\n",
    "\n",
    "### Example 1: Fill missing values with a specific value\n",
    "``` df['entries'] = df['entries'].fillna(0) ```\n",
    "\n",
    "### Example 2: Fill missing values with the column mean\n",
    "``` df['entries'] = df['entries'].fillna(df['entries'].mean()) ```\n",
    "\n",
    "### Example 3: Forward fill (use the previous value)\n",
    "``` df = df.fillna(method='ffill') ```\n",
    "\n",
    "### Example 4: Drop rows with any missing values\n",
    "``` df_clean = df.dropna() ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Detect and handle outliers\n",
    "# Using IQR method to identify outliers\n",
    "Q1 = df['entries'].quantile(0.25)\n",
    "Q3 = df['entries'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outliers as values outside 1.5 * IQR from Q1 or Q3\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"\\nOutlier boundaries for 'entries':\")\n",
    "print(f\"Lower bound: {lower_bound}\")\n",
    "print(f\"Upper bound: {upper_bound}\")\n",
    "\n",
    "# Count outliers\n",
    "outliers = df[(df['entries'] < lower_bound) | (df['entries'] > upper_bound)]\n",
    "print(f\"\\nNumber of outlier records: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Removing Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we want to remove unnecessary columns\n",
    "columns_to_drop = ['c/a', 'scp']\n",
    "df_slim = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Original columns: {df.columns.tolist()}\")\n",
    "print(f\"Columns after dropping: {df_slim.columns.tolist()}\")\n",
    "\n",
    "# Alternative methods:\n",
    "# df_slim = df.drop(['c/a', 'scp'], axis=1)  # Same result\n",
    "# To modify in place:\n",
    "# df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Does inplace=True Mean in Pandas?\n",
    "In Pandas, inplace=True is an argument used in various DataFrame methods to modify the data directly, without creating a new copy.\n",
    "\n",
    "How inplace=True Works\n",
    "- When inplace=False (default), the operation returns a new modified DataFrame, leaving the original unchanged.\n",
    "- When inplace=True, the operation modifies the DataFrame in place, meaning no new DataFrame is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample DataFrame\n",
    "# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "# # Without inplace=True (Creates a new DataFrame)\n",
    "# df_new = df.drop(columns=['B'])\n",
    "# print()\n",
    "# print(df_new)  # 'B' is removed\n",
    "# print(df)      # Original df is unchanged\n",
    "\n",
    "# # # With inplace=True (Modifies df directly)\n",
    "# # df.drop(columns=['B'], inplace=True)\n",
    "# # print(df)  # 'B' is permanently removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Analysis Summary and Potential Misleading Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've learned about the dataset\n",
    "print(\"SUMMARY OF ANALYSIS:\")\n",
    "print(f\"1. Dataset spans from {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"2. Contains data from {df['station'].nunique()} unique stations\")\n",
    "print(f\"3. Most common station: {df['station'].value_counts().idxmax()}\")\n",
    "print(f\"4. Weekend vs Weekday entries ratio: {weekend_comparison.loc['Weekend', 'entries'] / weekend_comparison.loc['Weekday', 'entries']:.2f}\")\n",
    "\n",
    "# Discussion of potentially misleading aspects in the visualizations\n",
    "print(\"\\nPOTENTIAL MISLEADING ASPECTS IN THE ANALYSIS:\")\n",
    "print(\"1. Day of week charts could be misleading because:\")\n",
    "print(\"   - Weekend days (2 days) are compared with weekday data (5 days)\")\n",
    "print(\"   - We're looking at totals, not averages per day\")\n",
    "print(\"   - Data might not account for holidays or special events\")\n",
    "print(\"2. Station traffic patterns might vary significantly by location\")\n",
    "print(\"3. Without normalizing for number of turnstiles, busier stations may be overrepresented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Exporting Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_slim.to_csv('processed_turnstile_data.csv', index=False)\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter('turnstile_analysis.xlsx') as writer:\n",
    "    df_slim.head(1000).to_excel(writer, sheet_name='Raw_Data', index=False)\n",
    "    day_grouped.to_excel(writer, sheet_name='Day_Analysis')\n",
    "    weekend_comparison.to_excel(writer, sheet_name='Weekend_Comparison')\n",
    "    \n",
    "print(\"Data exported to CSV and Excel files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV (default: index=True)\n",
    "``` df.to_csv('output.csv') ```\n",
    "\n",
    "Output (inside 'output.csv'):\n",
    ",A,B\n",
    "0,10,40\n",
    "1,20,50\n",
    "2,30,60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comprehensive pandas lab, we've learned:\n",
    "\n",
    "1. Data Import and Exploration:\n",
    "   - Reading CSV files and examining structure\n",
    "   - Checking data types and missing values\n",
    "\n",
    "2. Data Cleaning and Transformation:\n",
    "   - Renaming columns and standardizing names\n",
    "   - Converting data types for analysis\n",
    "   - Creating derived fields (day of week, time of day)\n",
    "\n",
    "3. Data Analysis Techniques:\n",
    "   - Using .map() for value mapping \n",
    "   - Using .apply() for complex transformations\n",
    "   - Grouping data for aggregation\n",
    "   - Sorting for exploratory analysis\n",
    "\n",
    "4. Index Manipulation:\n",
    "   - Setting and resetting indices\n",
    "   - Working with multi-level indices\n",
    "\n",
    "5. Visualization:\n",
    "   - Creating insightful plots\n",
    "   - Understanding potential misleading aspects\n",
    "\n",
    "The MTA turnstile dataset provides rich insights into usage patterns by day of week, time of day, and location. Such analysis can help optimize staffing, maintenance schedules, and improve passenger experience.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
